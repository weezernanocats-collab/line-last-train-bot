"""
çµ‚é›»æ¤œç´¢ LINE Bot

LINEã§é§…åã‚’é€ã‚‹ã¨ã€ãã®é§…ã‹ã‚‰è¥¿åƒè‘‰é§…ã¸ã®çµ‚é›»ã‚’èª¿ã¹ã¦è¿”ä¿¡ã™ã‚‹
"""

import os
import re
import json
import requests
from urllib.parse import quote
from datetime import datetime, timezone, timedelta
from flask import Flask, request, abort
from linebot.v3 import WebhookHandler
from linebot.v3.messaging import (
    Configuration,
    ApiClient,
    MessagingApi,
    ReplyMessageRequest,
    TextMessage,
)
from linebot.v3.webhooks import MessageEvent, TextMessageContent
from linebot.v3.exceptions import InvalidSignatureError
from bs4 import BeautifulSoup

app = Flask(__name__)

# ç’°å¢ƒå¤‰æ•°ã‹ã‚‰è¨­å®šã‚’å–å¾—
LINE_CHANNEL_ACCESS_TOKEN = os.environ.get("LINE_CHANNEL_ACCESS_TOKEN", "")
LINE_CHANNEL_SECRET = os.environ.get("LINE_CHANNEL_SECRET", "")

# LINE Bot SDK v3 ã®è¨­å®š
configuration = Configuration(access_token=LINE_CHANNEL_ACCESS_TOKEN)
handler = WebhookHandler(LINE_CHANNEL_SECRET)

# ç›®çš„åœ°
DESTINATION_STATION = "è¥¿åƒè‘‰"

# Yahoo!è·¯ç·šæƒ…å ±ï¼ˆé–¢æ±ã‚¨ãƒªã‚¢ï¼‰ã®é…å»¶æƒ…å ±URL
YAHOO_TRANSIT_URL = "https://transit.yahoo.co.jp/diainfo/area/3"

# è¥¿åƒè‘‰é§…ã«é–¢é€£ã™ã‚‹è·¯ç·šï¼ˆé…å»¶ãƒã‚§ãƒƒã‚¯å¯¾è±¡ï¼‰
RELATED_LINES = [
    "ç·æ­¦ç·š",
    "ç·æ­¦æœ¬ç·š",
    "ä¸­å¤®ãƒ»ç·æ­¦ç·š",
    "äº¬è‘‰ç·š",
    "æ­¦è”µé‡ç·š",
    "äº¬æˆç·š",
    "äº¬æˆåƒè‘‰ç·š",
]


@app.route("/")
def index():
    """ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯ç”¨"""
    return "LINE Last Train Bot is running!"


@app.route("/callback", methods=["POST"])
def callback():
    """LINE Webhookã®ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ"""
    signature = request.headers.get("X-Line-Signature", "")
    body = request.get_data(as_text=True)

    app.logger.info(f"Request body: {body}")

    try:
        handler.handle(body, signature)
    except InvalidSignatureError:
        app.logger.error("Invalid signature")
        abort(400)

    return "OK"


@handler.add(MessageEvent, message=TextMessageContent)
def handle_message(event):
    """ãƒ†ã‚­ã‚¹ãƒˆãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å—ä¿¡ã—ãŸã¨ãã®å‡¦ç†"""
    user_message = event.message.text.strip()
    app.logger.info(f"Received message: {user_message}")

    # é§…åã‚’æŠ½å‡ºï¼ˆã‚·ãƒ³ãƒ—ãƒ«ã«ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å…¨ä½“ã‚’é§…åã¨ã—ã¦æ‰±ã†ï¼‰
    station_name = extract_station_name(user_message)

    if not station_name:
        reply_text = "é§…åã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚\nä¾‹: æ±äº¬ã€ç§‹è‘‰åŸã€åƒè‘‰"
    else:
        # çµ‚é›»ã‚’æ¤œç´¢
        reply_text = search_last_train(station_name, DESTINATION_STATION)

    # è¿”ä¿¡
    with ApiClient(configuration) as api_client:
        messaging_api = MessagingApi(api_client)
        messaging_api.reply_message(
            ReplyMessageRequest(
                reply_token=event.reply_token,
                messages=[TextMessage(text=reply_text)]
            )
        )


def extract_station_name(message):
    """
    ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‹ã‚‰é§…åã‚’æŠ½å‡ºã™ã‚‹

    Args:
        message: ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸

    Returns:
        str: æŠ½å‡ºã—ãŸé§…åï¼ˆè¦‹ã¤ã‹ã‚‰ãªã‘ã‚Œã°Noneï¼‰
    """
    # ã€Œã€œé§…ã€ã¨ã„ã†å½¢å¼ãŒã‚ã‚Œã°æŠ½å‡º
    match = re.search(r"(.+?)é§…", message)
    if match:
        return match.group(1)

    # ã€Œã€œã‹ã‚‰ã€ã€Œã€œã§ã€ãªã©ã®ãƒ‘ã‚¿ãƒ¼ãƒ³
    patterns = [
        r"(.+?)ã‹ã‚‰(?:å¸°|çµ‚é›»|é›»è»Š)",
        r"(.+?)ã§(?:é£²|éŠ|ä»•äº‹)",
        r"^([ã-ã‚“ã‚¡-ãƒ³ä¸€-é¾¥a-zA-Z]+)$",  # é§…åã®ã¿
    ]

    for pattern in patterns:
        match = re.search(pattern, message)
        if match:
            station = match.group(1).strip()
            # çŸ­ã™ãã‚‹å ´åˆã¯é™¤å¤–
            if len(station) >= 1:
                return station

    # ãƒ‘ã‚¿ãƒ¼ãƒ³ã«ãƒãƒƒãƒã—ãªã‘ã‚Œã°ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å…¨ä½“ã‚’é§…åã¨ã—ã¦æ‰±ã†
    # ãŸã ã—é•·ã™ãã‚‹å ´åˆã¯é™¤å¤–
    if len(message) <= 10:
        return message

    return None


def search_last_train(from_station, to_station):
    """
    Yahoo!è·¯ç·šæƒ…å ±ã§çµ‚é›»ã‚’æ¤œç´¢ã™ã‚‹

    Args:
        from_station: å‡ºç™ºé§…
        to_station: åˆ°ç€é§…

    Returns:
        str: æ¤œç´¢çµæœã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
    """
    try:
        # æ—¥æœ¬æ™‚é–“ã§ä»Šæ—¥ã®æ—¥ä»˜ã‚’å–å¾—
        jst = timezone(timedelta(hours=9))
        now = datetime.now(jst)

        # é…å»¶æƒ…å ±ã‚’å–å¾—
        delays = fetch_delay_info()

        # Yahoo!è·¯ç·šæƒ…å ±ã®URL
        # type=1: å‡ºç™ºæ™‚åˆ»æŒ‡å®šã€æ·±å¤œ23:50ã§æ¤œç´¢ã—ã¦çµ‚é›»ã‚’å–å¾—
        url = (
            f"https://transit.yahoo.co.jp/search/result"
            f"?from={quote(from_station)}"
            f"&to={quote(to_station)}"
            f"&y={now.year}&m={now.month:02d}&d={now.day:02d}"
            f"&hh=23&mm=50"
            f"&type=1"  # å‡ºç™ºæ™‚åˆ»æŒ‡å®š
            f"&ticket=ic"  # ICå„ªå…ˆ
        )

        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
        }

        response = requests.get(url, headers=headers, timeout=15)
        response.raise_for_status()
        response.encoding = "utf-8"

        soup = BeautifulSoup(response.text, "html.parser")

        # ã‚¨ãƒ©ãƒ¼ãƒã‚§ãƒƒã‚¯ï¼ˆé§…ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆï¼‰
        error_elem = soup.select_one("div.elmErrorText, p.errTxt")
        if error_elem:
            return f"ã€Œ{from_station}ã€ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚\næ­£å¼ãªé§…åã§å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚"

        # å€™è£œé§…ãŒè¤‡æ•°ã‚ã‚‹å ´åˆ
        candidate_list = soup.select("div.candiList a, ul.candidate a")
        if candidate_list:
            candidates = [a.get_text(strip=True) for a in candidate_list[:5]]
            return f"ã€Œ{from_station}ã€ã«è©²å½“ã™ã‚‹é§…ãŒè¤‡æ•°ã‚ã‚Šã¾ã™:\n" + "\n".join(f"ãƒ»{c}" for c in candidates)

        # JSONãƒ‡ãƒ¼ã‚¿ã‹ã‚‰çµŒè·¯æƒ…å ±ã‚’å–å¾—
        result = parse_route_from_json(soup, from_station, to_station)

        if not result:
            # JSONãŒå–å¾—ã§ããªã„å ´åˆã¯HTMLã‹ã‚‰ãƒ‘ãƒ¼ã‚¹
            result = parse_route_result(soup, from_station, to_station)

        if not result:
            return f"çµ‚é›»æƒ…å ±ã‚’å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚\nYahoo!è·¯ç·šæƒ…å ±ã§ç›´æ¥æ¤œç´¢ã—ã¦ãã ã•ã„ã€‚"

        # é…å»¶æƒ…å ±ãŒã‚ã‚Œã°è¿½åŠ 
        if delays:
            delay_msg = format_delay_info(delays)
            result = result + "\n" + delay_msg

            # é…å»¶æ™‚ã¯ä»£æ›¿ãƒ«ãƒ¼ãƒˆã‚’æ¤œç´¢
            alt_result = search_alternative_route(from_station, to_station, now)
            if alt_result:
                result = result + "\n" + alt_result

        return result

    except requests.Timeout:
        return "æ¤œç´¢ãŒã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã—ã¾ã—ãŸã€‚\nã—ã°ã‚‰ãã—ã¦ã‹ã‚‰ã‚‚ã†ä¸€åº¦ãŠè©¦ã—ãã ã•ã„ã€‚"
    except requests.RequestException as e:
        app.logger.error(f"Request error: {e}")
        return "æ¤œç´¢ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚\nã—ã°ã‚‰ãã—ã¦ã‹ã‚‰ã‚‚ã†ä¸€åº¦ãŠè©¦ã—ãã ã•ã„ã€‚"


def format_delay_info(delays):
    """
    é…å»¶æƒ…å ±ã‚’ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã™ã‚‹

    Args:
        delays: é…å»¶æƒ…å ±ã®ãƒªã‚¹ãƒˆ

    Returns:
        str: ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã•ã‚ŒãŸé…å»¶æƒ…å ±
    """
    lines = [
        "",
        "âš ï¸ é‹è¡Œæƒ…å ± âš ï¸",
    ]

    for delay in delays[:3]:  # æœ€å¤§3ä»¶
        lines.append(f"ğŸ”´ {delay['line']}")
        lines.append(f"   {delay['status'][:50]}")

    return "\n".join(lines)


def search_alternative_route(from_station, to_station, now):
    """
    ä»£æ›¿ãƒ«ãƒ¼ãƒˆã‚’æ¤œç´¢ã™ã‚‹ï¼ˆé…å»¶æ™‚ç”¨ï¼‰

    Args:
        from_station: å‡ºç™ºé§…
        to_station: åˆ°ç€é§…
        now: ç¾åœ¨æ™‚åˆ»

    Returns:
        str: ä»£æ›¿ãƒ«ãƒ¼ãƒˆæƒ…å ±ï¼ˆãªã‘ã‚Œã°Noneï¼‰
    """
    try:
        # çµŒç”±é§…ã‚’å¤‰ãˆã¦æ¤œç´¢ï¼ˆä¾‹ï¼šæ±äº¬çµŒç”±ã€èˆ¹æ©‹çµŒç”±ãªã©ï¼‰
        via_stations = ["èˆ¹æ©‹", "æ´¥ç”°æ²¼", "åƒè‘‰"]

        for via in via_stations:
            if via == from_station or via == to_station:
                continue

            url = (
                f"https://transit.yahoo.co.jp/search/result"
                f"?from={quote(from_station)}"
                f"&to={quote(to_station)}"
                f"&via={quote(via)}"
                f"&y={now.year}&m={now.month:02d}&d={now.day:02d}"
                f"&hh=23&mm=30"
                f"&type=1"
                f"&ticket=ic"
            )

            headers = {
                "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
            }

            response = requests.get(url, headers=headers, timeout=10)
            if response.status_code == 200:
                soup = BeautifulSoup(response.text, "html.parser")

                # æ™‚åˆ»ã‚’å–å¾—
                time_match = re.search(r"(\d{1,2}:\d{2})ç™º.*?(\d{1,2}:\d{2})ç€", soup.get_text())
                if time_match:
                    return f"\nğŸ’¡ ä»£æ›¿ãƒ«ãƒ¼ãƒˆï¼ˆ{via}çµŒç”±ï¼‰\n   ç™ºè»Š {time_match.group(1)} â†’ åˆ°ç€ {time_match.group(2)}"

    except Exception as e:
        app.logger.error(f"Alternative route search error: {e}")

    return None


def fetch_delay_info():
    """
    Yahoo!è·¯ç·šæƒ…å ±ã‹ã‚‰é–¢é€£è·¯ç·šã®é…å»¶æƒ…å ±ã‚’å–å¾—ã™ã‚‹

    Returns:
        list: é…å»¶æƒ…å ±ã®ãƒªã‚¹ãƒˆ [{"line": è·¯ç·šå, "status": çŠ¶æ³}]
    """
    delays = []

    try:
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
        }
        response = requests.get(YAHOO_TRANSIT_URL, headers=headers, timeout=10)
        response.raise_for_status()
        response.encoding = response.apparent_encoding

        soup = BeautifulSoup(response.text, "html.parser")

        # é‹è¡Œæƒ…å ±ã®ãƒªã‚¹ãƒˆã‚’å–å¾—
        trouble_items = soup.select("div.trouble a, li.elmTblLstLine a")

        for item in trouble_items:
            line_name = item.get_text(strip=True)

            # é–¢é€£è·¯ç·šã‹ãƒã‚§ãƒƒã‚¯
            for target in RELATED_LINES:
                if target in line_name:
                    # è©³ç´°ãƒšãƒ¼ã‚¸ã®URLã‚’å–å¾—
                    detail_url = item.get("href")
                    if detail_url and not detail_url.startswith("http"):
                        detail_url = "https://transit.yahoo.co.jp" + detail_url

                    status = fetch_delay_detail(detail_url) if detail_url else "é‹è¡Œæƒ…å ±ã‚ã‚Š"

                    delays.append({
                        "line": line_name,
                        "status": status
                    })
                    break

    except requests.RequestException as e:
        app.logger.error(f"é…å»¶æƒ…å ±ã®å–å¾—ã«å¤±æ•—: {e}")

    return delays


def fetch_delay_detail(url):
    """
    è©³ç´°ãƒšãƒ¼ã‚¸ã‹ã‚‰é…å»¶çŠ¶æ³ã‚’å–å¾—ã™ã‚‹

    Args:
        url: è©³ç´°ãƒšãƒ¼ã‚¸ã®URL

    Returns:
        str: é…å»¶çŠ¶æ³ã®è©³ç´°
    """
    try:
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
        }
        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status()
        response.encoding = response.apparent_encoding

        soup = BeautifulSoup(response.text, "html.parser")

        # çŠ¶æ³ã®è©³ç´°ã‚’å–å¾—
        status_elem = soup.select_one("div.trouble p, p.trouble, div.statusTxt")
        if status_elem:
            return status_elem.get_text(strip=True)[:100]  # 100æ–‡å­—ã¾ã§

    except requests.RequestException:
        pass

    return "è©³ç´°æƒ…å ±å–å¾—å¤±æ•—"


def parse_route_from_json(soup, from_station, to_station):
    """
    ãƒšãƒ¼ã‚¸å†…ã®JSONãƒ‡ãƒ¼ã‚¿ã‹ã‚‰çµŒè·¯æƒ…å ±ã‚’å–å¾—ã™ã‚‹

    Args:
        soup: BeautifulSoupã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ
        from_station: å‡ºç™ºé§…
        to_station: åˆ°ç€é§…

    Returns:
        str: ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã•ã‚ŒãŸçµæœãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
    """
    try:
        # scriptã‚¿ã‚°ã‹ã‚‰JSONãƒ‡ãƒ¼ã‚¿ã‚’æ¢ã™
        scripts = soup.find_all("script")
        route_data = None

        for script in scripts:
            if script.string and "featureInfoList" in script.string:
                # naviSearchParam ã‚’æ¢ã™
                match = re.search(r'naviSearchParam\s*=\s*(\{.+?\});', script.string, re.DOTALL)
                if match:
                    try:
                        route_data = json.loads(match.group(1))
                        break
                    except json.JSONDecodeError:
                        continue

        if not route_data:
            return None

        # æœ€åˆã®ãƒ«ãƒ¼ãƒˆï¼ˆçµ‚é›»ã«æœ€ã‚‚è¿‘ã„ï¼‰ã‚’å–å¾—
        feature_list = route_data.get("featureInfoList", [])
        edge_list = route_data.get("edgeInfoList", [])

        if not feature_list or not edge_list:
            return None

        # æœ€åˆã®ãƒ«ãƒ¼ãƒˆã®æƒ…å ±
        feature = feature_list[0]
        edges = edge_list[0] if edge_list else []

        # ç™ºè»Šãƒ»åˆ°ç€æ™‚åˆ»ã‚’å–å¾—ï¼ˆHH:MMå½¢å¼ã®ã¿ï¼‰
        dep_time = feature.get("departureTime", "")
        arr_time = feature.get("arrivalTime", "")

        # æ™‚åˆ»ã‹ã‚‰HH:MMéƒ¨åˆ†ã®ã¿æŠ½å‡º
        time_match = re.search(r"(\d{1,2}:\d{2})", dep_time)
        if time_match:
            dep_time = time_match.group(1)

        time_match = re.search(r"(\d{1,2}:\d{2})", arr_time)
        if time_match:
            arr_time = time_match.group(1)

        # ä¹—æ›å›æ•°
        transfer_count = feature.get("transferCount", 0)

        # è·¯ç·šæƒ…å ±ã¨ç•ªç·šã‚’å–å¾—
        route_details = []
        for edge in edges:
            if isinstance(edge, dict):
                rail_name = edge.get("railName", "")
                # ç•ªç·šæƒ…å ±
                riding_info = edge.get("ridingPositionInfo", {})
                if riding_info:
                    dep_platform = riding_info.get("departure", "")
                    if rail_name and dep_platform:
                        route_details.append(f"{rail_name}ï¼ˆ{dep_platform}ï¼‰")
                    elif rail_name:
                        route_details.append(rail_name)
                elif rail_name:
                    route_details.append(rail_name)

        # çµæœã‚’ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ
        lines = [
            f"ğŸšƒ {from_station} â†’ {to_station}",
            "",
            f"ç™ºè»Š {dep_time}",
            f"åˆ°ç€ {arr_time}",
        ]

        if transfer_count > 0:
            lines.append(f"ä¹—æ› {transfer_count}å›")

        if route_details:
            lines.append("")
            for detail in route_details[:3]:  # æœ€å¤§3è·¯ç·š
                lines.append(f"â–¶ {detail}")

        lines.extend([
            "",
            "â€» é‹è¡ŒçŠ¶æ³ã«ã‚ˆã‚Šå¤‰æ›´ã®å ´åˆã‚ã‚Š",
        ])

        return "\n".join(lines)

    except Exception as e:
        app.logger.error(f"JSON parse error: {e}")
        return None


def parse_route_result(soup, from_station, to_station):
    """
    æ¤œç´¢çµæœãƒšãƒ¼ã‚¸ã®HTMLã‹ã‚‰çµ‚é›»æƒ…å ±ã‚’ãƒ‘ãƒ¼ã‚¹ã™ã‚‹ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç”¨ï¼‰

    Args:
        soup: BeautifulSoupã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ
        from_station: å‡ºç™ºé§…
        to_station: åˆ°ç€é§…

    Returns:
        str: ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã•ã‚ŒãŸçµæœãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
    """
    try:
        # æ™‚åˆ»ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆHH:MMå½¢å¼ï¼‰ã‚’æ¢ã™
        time_pattern = re.compile(r"(\d{1,2}:\d{2})ç™º")
        arr_pattern = re.compile(r"(\d{1,2}:\d{2})ç€")

        all_text = soup.get_text()

        dep_match = time_pattern.search(all_text)
        arr_match = arr_pattern.search(all_text)

        dep_time = dep_match.group(1) if dep_match else None
        arr_time = arr_match.group(1) if arr_match else None

        if not dep_time:
            # åˆ¥ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’è©¦ã™
            times = re.findall(r"(\d{1,2}:\d{2})", all_text)
            if len(times) >= 2:
                dep_time = times[0]
                arr_time = times[1]

        # çµæœã‚’ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ
        if dep_time:
            lines = [
                f"ğŸšƒ {from_station} â†’ {to_station}",
                "",
                f"ç™ºè»Š {dep_time}",
            ]

            if arr_time:
                lines.append(f"åˆ°ç€ {arr_time}")

            lines.extend([
                "",
                "â€» è©³ç´°ã¯Yahoo!è·¯ç·šæƒ…å ±ã§ç¢ºèªã—ã¦ãã ã•ã„",
            ])

            return "\n".join(lines)

        return None

    except Exception as e:
        app.logger.error(f"Parse error: {e}")
        return None


if __name__ == "__main__":
    port = int(os.environ.get("PORT", 5000))
    app.run(host="0.0.0.0", port=port, debug=False)
